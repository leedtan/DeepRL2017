3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) 
[GCC 7.2.0]
using old dagger data
using pre trained model
learning rate: 0.04472135955
pre_adv_loss 0.0617490261295 post_adv_loss 0.0631579960894 diff 0.00140896995991
Epoch:  4  avg train loss: 0.0630337264578  Reg Loss: 4.72127813973e-05 total time: 7.041403293609619
avg loss validation: 0.0129549960955 failed count: 0
learning rate: 0.0408248290464
pre_adv_loss 0.0107005146693 post_adv_loss 0.0116448027265 diff 0.000944288057205
Epoch:  5  avg train loss: 0.0116360896683  Reg Loss: 2.43691673649e-05 total time: 16.812492609024048
avg loss validation: 0.00967398096928 failed count: 0
learning rate: 0.0377964473009
pre_adv_loss 0.00730399573706 post_adv_loss 0.00808147505071 diff 0.000777479313648
Epoch:  6  avg train loss: 0.00807593442569  Reg Loss: 1.92924370055e-05 total time: 26.08657193183899
avg loss validation: 0.00785367178221 failed count: 0
learning rate: 0.0353553390593
pre_adv_loss 0.00588147904094 post_adv_loss 0.00661245880331 diff 0.000730979762368
Epoch:  7  avg train loss: 0.00661183275264  Reg Loss: 1.68284103922e-05 total time: 35.64576458930969
avg loss validation: 0.00519075449217 failed count: 0
learning rate: 0.0333333333333
pre_adv_loss 0.00477961470666 post_adv_loss 0.00545344637125 diff 0.000673831664591
Epoch:  8  avg train loss: 0.00545582142021  Reg Loss: 1.51685714976e-05 total time: 44.90343713760376
avg loss validation: 0.00386410214399 failed count: 0
learning rate: 0.0316227766017
pre_adv_loss 0.00419445211375 post_adv_loss 0.00482993922398 diff 0.000635487110236
Epoch:  9  avg train loss: 0.00483211094847  Reg Loss: 1.39235682454e-05 total time: 54.25012135505676
avg loss validation: 0.00410275394295 failed count: 1
learning rate: 0.0301511344578
pre_adv_loss 0.00392195256158 post_adv_loss 0.00453886964138 diff 0.000616917079799
Epoch:  10  avg train loss: 0.00453650262545  Reg Loss: 1.29473940344e-05 total time: 60.286935329437256
avg loss validation: 0.00280012771207 failed count: 0
learning rate: 0.0288675134595
pre_adv_loss 0.00323914478007 post_adv_loss 0.00380217293654 diff 0.000563028156475
Epoch:  11  avg train loss: 0.00379915762817  Reg Loss: 1.20900144215e-05 total time: 69.9642379283905
avg loss validation: 0.00259389833011 failed count: 0
learning rate: 0.0277350098113
pre_adv_loss 0.0030852746497 post_adv_loss 0.00364697314099 diff 0.000561698491287
Epoch:  12  avg train loss: 0.0036475617964  Reg Loss: 1.13650540466e-05 total time: 79.82831382751465
avg loss validation: 0.00331649853514 failed count: 1
learning rate: 0.0267261241912
pre_adv_loss 0.00300200325047 post_adv_loss 0.0035380319917 diff 0.000536028741229
Epoch:  13  avg train loss: 0.0035393047218  Reg Loss: 1.07333454715e-05 total time: 86.48100876808167
avg loss validation: 0.00226068013329 failed count: 0
learning rate: 0.0258198889747
pre_adv_loss 0.00290684789053 post_adv_loss 0.00344703893071 diff 0.000540191040178
Epoch:  14  avg train loss: 0.00344546872577  Reg Loss: 1.01980958239e-05 total time: 96.17173171043396
avg loss validation: 0.00321372323129 failed count: 1
learning rate: 0.025
pre_adv_loss 0.00250469955429 post_adv_loss 0.00301939653498 diff 0.000514696980692
Epoch:  15  avg train loss: 0.00301623161299  Reg Loss: 9.70569288583e-06 total time: 102.3393805027008
avg loss validation: 0.00338090647326 failed count: 2
learning rate: 0.0242535625036
pre_adv_loss 0.00258334920906 post_adv_loss 0.00309352007004 diff 0.000510170860983
Epoch:  16  avg train loss: 0.00309363088419  Reg Loss: 9.28121009701e-06 total time: 108.38269853591919
avg loss validation: 0.00163515015928 failed count: 0
learning rate: 0.0235702260396
pre_adv_loss 0.00231135965458 post_adv_loss 0.00280076496147 diff 0.000489405306888
Epoch:  17  avg train loss: 0.00279991986829  Reg Loss: 8.88491571956e-06 total time: 117.70991253852844
avg loss validation: 0.00235107505668 failed count: 1
learning rate: 0.0229415733871
pre_adv_loss 0.00237275843644 post_adv_loss 0.00287484906269 diff 0.000502090626248
Epoch:  18  avg train loss: 0.00287350898357  Reg Loss: 8.52778961165e-06 total time: 123.786780834198
avg loss validation: 0.00271750899912 failed count: 2
learning rate: 0.022360679775
pre_adv_loss 0.00206340317624 post_adv_loss 0.00252600578252 diff 0.000462602606283
Epoch:  19  avg train loss: 0.00252459892731  Reg Loss: 8.2103849942e-06 total time: 129.83364963531494
avg loss validation: 0.00115705140194 failed count: 0
learning rate: 0.0218217890236
pre_adv_loss 0.00230169385797 post_adv_loss 0.00279267417634 diff 0.000490980318376
Epoch:  20  avg train loss: 0.00279115778809  Reg Loss: 7.92143645958e-06 total time: 139.73835015296936
avg loss validation: 0.00191707543412 failed count: 1
learning rate: 0.0213200716356
pre_adv_loss 0.0020291856056 post_adv_loss 0.00249091148911 diff 0.000461725883503
Epoch:  21  avg train loss: 0.00248993218289  Reg Loss: 7.66991988436e-06 total time: 145.78608059883118
avg loss validation: 0.00138181067621 failed count: 2
learning rate: 0.0208514414057
pre_adv_loss 0.00211389800783 post_adv_loss 0.00259059282643 diff 0.000476694818602
Epoch:  22  avg train loss: 0.00259078986636  Reg Loss: 7.44951623445e-06 total time: 151.84846115112305
avg loss validation: 0.00193129579759 failed count: 3
loading and building expert policy
obs (1, 11) (1, 11)
loaded and built
threshold: 0
iter 0
100/1000
200/1000
300/1000
400/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
returns [1753.557256945905, 3785.8676942823754, 3793.1129318682842, 1976.0441358361127, 2287.3614785103819]
mean return 2719.18869949
std of return 890.202225218
DAGGER NUMBER: 0 of: 200
learning rate: 0.0204124145232
pre_adv_loss 0.00208021947183 post_adv_loss 0.00256212417702 diff 0.000481904705197
Epoch:  23  avg train loss: 0.00256071498237  Reg Loss: 7.91646176275e-06 total time: 6.890548229217529
avg loss validation: 0.00133511385137 failed count: 0
learning rate: 0.02
pre_adv_loss 0.00216125292904 post_adv_loss 0.002647762614 diff 0.000486509684965
Epoch:  24  avg train loss: 0.00264666838025  Reg Loss: 7.6648481481e-06 total time: 16.822483777999878
avg loss validation: 0.00195393377476 failed count: 1
learning rate: 0.0196116135138
pre_adv_loss 0.00181179516868 post_adv_loss 0.00225734641135 diff 0.000445551242673
Epoch:  25  avg train loss: 0.00225909530528  Reg Loss: 7.43221852954e-06 total time: 23.227030038833618
avg loss validation: 0.00195749723595 failed count: 2
learning rate: 0.019245008973
pre_adv_loss 0.00176043765128 post_adv_loss 0.00220615090156 diff 0.000445713250277
Epoch:  26  avg train loss: 0.00220740235191  Reg Loss: 7.20168415612e-06 total time: 29.73290705680847
avg loss validation: 0.00148835281642 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
iter 3
100/1000
200/1000
300/1000
400/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [1765.9549512615395, 3792.45276888131, 1917.4822211268461, 1769.6466165081665, 3794.1214020380025]
mean return 2607.93159196
std of return 969.381594765
DAGGER NUMBER: 1 of: 200
learning rate: 0.0188982236505
pre_adv_loss 0.00190184832417 post_adv_loss 0.0023667100088 diff 0.000464861684623
Epoch:  27  avg train loss: 0.00236670229904  Reg Loss: 7.66420813441e-06 total time: 7.425880670547485
avg loss validation: 0.0011641226612 failed count: 0
learning rate: 0.0185695338177
pre_adv_loss 0.00185061663104 post_adv_loss 0.00230505758552 diff 0.000454440954475
Epoch:  28  avg train loss: 0.00230476699321  Reg Loss: 7.41708300965e-06 total time: 17.7245135307312
avg loss validation: 0.00121471599912 failed count: 1
learning rate: 0.0182574185835
pre_adv_loss 0.00188751953992 post_adv_loss 0.00233423488182 diff 0.000446715341902
Epoch:  29  avg train loss: 0.00233403378889  Reg Loss: 7.21170276294e-06 total time: 24.304256916046143
avg loss validation: 0.00156399814211 failed count: 2
learning rate: 0.0179605302027
pre_adv_loss 0.00166532314192 post_adv_loss 0.0020972218912 diff 0.00043189874928
Epoch:  30  avg train loss: 0.00209718869974  Reg Loss: 7.00539556791e-06 total time: 30.845505475997925
avg loss validation: 0.000967590346641 failed count: 0
learning rate: 0.0176776695297
pre_adv_loss 0.00172834714951 post_adv_loss 0.00215933464043 diff 0.000430987490918
Epoch:  31  avg train loss: 0.0021590262604  Reg Loss: 6.81724715402e-06 total time: 40.67918014526367
avg loss validation: 0.000951806021193 failed count: 0
learning rate: 0.0174077655956
pre_adv_loss 0.00166149943616 post_adv_loss 0.00209099409715 diff 0.000429494660983
Epoch:  32  avg train loss: 0.00209105130381  Reg Loss: 6.65233680527e-06 total time: 50.510398864746094
avg loss validation: 0.00218818780612 failed count: 1
learning rate: 0.0171498585143
pre_adv_loss 0.00168490574249 post_adv_loss 0.00211905583367 diff 0.000434150091182
Epoch:  33  avg train loss: 0.00211876171592  Reg Loss: 6.52153166703e-06 total time: 57.232760429382324
avg loss validation: 0.000840571136938 failed count: 0
learning rate: 0.0169030850946
pre_adv_loss 0.00150448931095 post_adv_loss 0.00190762444748 diff 0.000403135136532
Epoch:  34  avg train loss: 0.00190770989801  Reg Loss: 6.37826149887e-06 total time: 67.82110834121704
avg loss validation: 0.00131229652832 failed count: 1
learning rate: 0.0166666666667
pre_adv_loss 0.00150826807634 post_adv_loss 0.00192376894983 diff 0.000415500873491
Epoch:  35  avg train loss: 0.00192387006502  Reg Loss: 6.24121927999e-06 total time: 75.68492841720581
avg loss validation: 0.00141686143976 failed count: 2
learning rate: 0.0164398987305
pre_adv_loss 0.00147453091343 post_adv_loss 0.00189017141558 diff 0.000415640502152
Epoch:  36  avg train loss: 0.00189043126048  Reg Loss: 6.13293885176e-06 total time: 82.30341386795044
avg loss validation: 0.0012011159026 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3784.311239796732, 3792.5587205110687, 3792.5130121268517, 3788.4585641706085, 3797.731936766213]
mean return 3791.11469467
std of return 4.49823126889
DAGGER NUMBER: 2 of: 200
learning rate: 0.0162221421131
pre_adv_loss 0.0014814935462 post_adv_loss 0.00188748584794 diff 0.000405992301739
Epoch:  37  avg train loss: 0.00188685860049  Reg Loss: 6.37118832015e-06 total time: 7.137088298797607
avg loss validation: 0.000966453908757 failed count: 0
learning rate: 0.0160128153805
pre_adv_loss 0.00146665900937 post_adv_loss 0.00188647014114 diff 0.000419811131764
Epoch:  38  avg train loss: 0.00188654009672  Reg Loss: 6.23283320518e-06 total time: 17.689738273620605
avg loss validation: 0.00127092126253 failed count: 1
learning rate: 0.0158113883008
pre_adv_loss 0.00141898057697 post_adv_loss 0.00182216703677 diff 0.0004031864598
Epoch:  39  avg train loss: 0.00182236073491  Reg Loss: 6.11271292844e-06 total time: 24.84306025505066
avg loss validation: 0.00137535253443 failed count: 2
learning rate: 0.0156173761889
pre_adv_loss 0.00136617309225 post_adv_loss 0.00176523457374 diff 0.000399061481489
Epoch:  40  avg train loss: 0.00176430750704  Reg Loss: 5.98711879478e-06 total time: 31.919658184051514
avg loss validation: 0.000957571313483 failed count: 0
learning rate: 0.0154303349962
pre_adv_loss 0.00123135868606 post_adv_loss 0.00160480428869 diff 0.000373445602629
Epoch:  41  avg train loss: 0.0016047979859  Reg Loss: 5.87456137315e-06 total time: 42.53073692321777
avg loss validation: 0.00156109249561 failed count: 1
learning rate: 0.0152498570333
pre_adv_loss 0.00135404466617 post_adv_loss 0.00174986938211 diff 0.000395824715943
Epoch:  42  avg train loss: 0.00174986746317  Reg Loss: 5.76985218382e-06 total time: 49.51490807533264
avg loss validation: 0.00192405529313 failed count: 2
learning rate: 0.0150755672289
pre_adv_loss 0.00119908052481 post_adv_loss 0.0015712578653 diff 0.000372177340492
Epoch:  43  avg train loss: 0.00157157622071  Reg Loss: 5.67145139023e-06 total time: 56.58400630950928
avg loss validation: 0.00112520027833 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3795.4558103860163, 2253.7806679221217, 3801.1588685631932, 3795.3327234372196, 3790.7494649053551]
mean return 3487.29550704
std of return 616.766255857
DAGGER NUMBER: 3 of: 200
learning rate: 0.01490711985
pre_adv_loss 0.00127208060664 post_adv_loss 0.00165805925441 diff 0.000385978647764
Epoch:  44  avg train loss: 0.00165786260025  Reg Loss: 5.88373389579e-06 total time: 8.213726282119751
avg loss validation: 0.00109172990713 failed count: 0
learning rate: 0.0147441956155
pre_adv_loss 0.00113422083164 post_adv_loss 0.00149644567312 diff 0.000362224841475
Epoch:  45  avg train loss: 0.00149652294649  Reg Loss: 5.76304082863e-06 total time: 19.571973085403442
avg loss validation: 0.0010462302583 failed count: 0
learning rate: 0.0145864991498
pre_adv_loss 0.00128336726626 post_adv_loss 0.00166838984739 diff 0.000385022581127
Epoch:  46  avg train loss: 0.001668040707  Reg Loss: 5.66038652163e-06 total time: 30.492733001708984
avg loss validation: 0.00100010571726 failed count: 0
learning rate: 0.0144337567297
pre_adv_loss 0.00125678347488 post_adv_loss 0.00163039612474 diff 0.000373612649862
Epoch:  47  avg train loss: 0.00163068031881  Reg Loss: 5.57153071031e-06 total time: 42.863959074020386
avg loss validation: 0.000793997935594 failed count: 0
learning rate: 0.0142857142857
pre_adv_loss 0.00128149569565 post_adv_loss 0.00167256480985 diff 0.000391069114199
Epoch:  48  avg train loss: 0.00167245157483  Reg Loss: 5.49187976551e-06 total time: 54.59420156478882
avg loss validation: 0.00129978960836 failed count: 1
learning rate: 0.0141421356237
pre_adv_loss 0.00114593575434 post_adv_loss 0.0015203154758 diff 0.000374379721465
Epoch:  49  avg train loss: 0.00152067821471  Reg Loss: 5.40596212225e-06 total time: 62.081971645355225
avg loss validation: 0.00103485409777 failed count: 2
learning rate: 0.0140028008403
pre_adv_loss 0.00117749781739 post_adv_loss 0.00154835272565 diff 0.000370854908264
Epoch:  50  avg train loss: 0.00154864263935  Reg Loss: 5.32213181245e-06 total time: 69.35540652275085
avg loss validation: 0.000989639463472 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3793.1898849474819, 3790.8689945276619, 3798.0405547336077, 3792.4979804069812, 3790.2128172910684]
mean return 3792.96204638
std of return 2.75672419287
DAGGER NUMBER: 4 of: 200
learning rate: 0.0138675049056
pre_adv_loss 0.00115592940832 post_adv_loss 0.00152035851213 diff 0.000364429103806
Epoch:  51  avg train loss: 0.00152007907282  Reg Loss: 5.48100401284e-06 total time: 7.625164985656738
avg loss validation: 0.000780764482327 failed count: 0
learning rate: 0.0137360563949
pre_adv_loss 0.00115649954993 post_adv_loss 0.0015284795685 diff 0.000371980018573
Epoch:  52  avg train loss: 0.00152819237171  Reg Loss: 5.38603642274e-06 total time: 18.488996028900146
avg loss validation: 0.00110157158896 failed count: 1
learning rate: 0.0136082763488
pre_adv_loss 0.00116528893258 post_adv_loss 0.00153962541466 diff 0.000374336482085
Epoch:  53  avg train loss: 0.0015391080342  Reg Loss: 5.30692627922e-06 total time: 26.14902687072754
avg loss validation: 0.000902078508934 failed count: 2
learning rate: 0.0134839972493
pre_adv_loss 0.00104469076411 post_adv_loss 0.00139713876767 diff 0.000352448003562
Epoch:  54  avg train loss: 0.00139704112312  Reg Loss: 5.22538946599e-06 total time: 33.763327836990356
avg loss validation: 0.000869374097148 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3798.4332152788497, 3790.9353788062376, 3800.1243761090036, 3788.3700236509253, 3789.9838015063306]
mean return 3793.56935907
std of return 4.76345482763
DAGGER NUMBER: 5 of: 200
learning rate: 0.0133630620956
pre_adv_loss 0.00119488380519 post_adv_loss 0.00157208163068 diff 0.000377197825492
Epoch:  55  avg train loss: 0.00157179455206  Reg Loss: 5.38143526814e-06 total time: 8.391669511795044
avg loss validation: 0.000727693777148 failed count: 0
learning rate: 0.0132453235707
pre_adv_loss 0.00107992298936 post_adv_loss 0.00143736660341 diff 0.000357443614052
Epoch:  56  avg train loss: 0.00143739576809  Reg Loss: 5.29404845706e-06 total time: 19.41277241706848
avg loss validation: 0.00123014423859 failed count: 1
learning rate: 0.013130643286
pre_adv_loss 0.00115584499225 post_adv_loss 0.00152828786919 diff 0.000372442876934
Epoch:  57  avg train loss: 0.00152802952046  Reg Loss: 5.2163927933e-06 total time: 27.280325889587402
avg loss validation: 0.000812892039478 failed count: 2
learning rate: 0.0130188910981
pre_adv_loss 0.00100180284843 post_adv_loss 0.00134925653936 diff 0.00034745369093
Epoch:  58  avg train loss: 0.00134839421398  Reg Loss: 5.13783905982e-06 total time: 35.15524864196777
avg loss validation: 0.000974269661394 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3794.8669286517761, 3785.0614518653661, 3794.0330457475266, 3797.8951011995982, 3791.2058890934491]
mean return 3792.61248331
std of return 4.33596947345
DAGGER NUMBER: 6 of: 200
learning rate: 0.0129099444874
pre_adv_loss 0.00104876208704 post_adv_loss 0.00139185385012 diff 0.000343091763071
Epoch:  59  avg train loss: 0.00139173347823  Reg Loss: 5.30247334469e-06 total time: 9.325411796569824
avg loss validation: 0.000635869883964 failed count: 0
learning rate: 0.0128036879933
pre_adv_loss 0.001011275557 post_adv_loss 0.00136545786698 diff 0.000354182309975
Epoch:  60  avg train loss: 0.00136537031642  Reg Loss: 5.21293061279e-06 total time: 21.812880039215088
avg loss validation: 0.00120558346609 failed count: 1
learning rate: 0.0127000127
pre_adv_loss 0.000985266487134 post_adv_loss 0.00132974967654 diff 0.000344483189405
Epoch:  61  avg train loss: 0.00132961347957  Reg Loss: 5.12625916004e-06 total time: 30.20548105239868
avg loss validation: 0.00115300144576 failed count: 2
learning rate: 0.012598815767
pre_adv_loss 0.0011369655186 post_adv_loss 0.00149666689431 diff 0.000359701375707
Epoch:  62  avg train loss: 0.00149626843933  Reg Loss: 5.04978782695e-06 total time: 38.44312930107117
avg loss validation: 0.000765608224764 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3787.3673762645585, 3787.6018499593843, 3793.3344632569674, 3786.9375235525199, 3791.5008187198196]
mean return 3789.34840635
std of return 2.58103947911
DAGGER NUMBER: 7 of: 200
learning rate: 0.0125
pre_adv_loss 0.00103392713513 post_adv_loss 0.00138020709567 diff 0.000346279960542
Epoch:  63  avg train loss: 0.00137978218202  Reg Loss: 5.2150338864e-06 total time: 8.6093270778656
avg loss validation: 0.00086665616663 failed count: 0
learning rate: 0.0124034734589
pre_adv_loss 0.000977830456145 post_adv_loss 0.0013183113413 diff 0.000340480885154
Epoch:  64  avg train loss: 0.00131819343651  Reg Loss: 5.12573751318e-06 total time: 21.007068872451782
avg loss validation: 0.00100644863624 failed count: 1
learning rate: 0.0123091490979
pre_adv_loss 0.00101919921407 post_adv_loss 0.00135928760574 diff 0.000340088391672
Epoch:  65  avg train loss: 0.00135898137122  Reg Loss: 5.04489681122e-06 total time: 29.556305646896362
avg loss validation: 0.000807898694416 failed count: 0
learning rate: 0.0122169444356
pre_adv_loss 0.000956841612859 post_adv_loss 0.0012912211104 diff 0.000334379497544
Epoch:  66  avg train loss: 0.00129220532563  Reg Loss: 4.96577637326e-06 total time: 41.782254219055176
avg loss validation: 0.0013329037912 failed count: 1
learning rate: 0.0121267812518
pre_adv_loss 0.000998167464637 post_adv_loss 0.00134607472316 diff 0.000347907258521
Epoch:  67  avg train loss: 0.00134553804793  Reg Loss: 4.89369312224e-06 total time: 50.32137989997864
avg loss validation: 0.000701287509621 failed count: 0
learning rate: 0.0120385853086
pre_adv_loss 0.000954580712802 post_adv_loss 0.00129389568539 diff 0.000339314972585
Epoch:  68  avg train loss: 0.00129395013329  Reg Loss: 4.82187166539e-06 total time: 62.13996887207031
avg loss validation: 0.00064395060009 failed count: 0
learning rate: 0.0119522860933
pre_adv_loss 0.000918580067786 post_adv_loss 0.0012486583085 diff 0.000330078240711
Epoch:  69  avg train loss: 0.00124820492438  Reg Loss: 4.75638476484e-06 total time: 74.52175879478455
avg loss validation: 0.0007821346604 failed count: 1
learning rate: 0.0118678165819
pre_adv_loss 0.000896561778898 post_adv_loss 0.00122494386549 diff 0.000328382086588
Epoch:  70  avg train loss: 0.00122452311318  Reg Loss: 4.68837031991e-06 total time: 83.06268429756165
avg loss validation: 0.000829391290328 failed count: 2
learning rate: 0.0117851130198
pre_adv_loss 0.000885340792235 post_adv_loss 0.00120769947033 diff 0.0003223586781
Epoch:  71  avg train loss: 0.00120741459769  Reg Loss: 4.6221059485e-06 total time: 91.6045515537262
avg loss validation: 0.000567297065524 failed count: 0
learning rate: 0.0117041147196
pre_adv_loss 0.000829191105636 post_adv_loss 0.00114236490087 diff 0.000313173795236
Epoch:  72  avg train loss: 0.00114200406825  Reg Loss: 4.56124714122e-06 total time: 103.48107433319092
avg loss validation: 0.000711024848999 failed count: 1
learning rate: 0.0116247638744
pre_adv_loss 0.000828154836676 post_adv_loss 0.0011420145201 diff 0.00031385968342
Epoch:  73  avg train loss: 0.00114165670847  Reg Loss: 4.50154207105e-06 total time: 112.02070260047913
avg loss validation: 0.000828792630551 failed count: 2
learning rate: 0.0115470053838
pre_adv_loss 0.000836899087487 post_adv_loss 0.00114956309174 diff 0.000312664004252
Epoch:  74  avg train loss: 0.00114908790434  Reg Loss: 4.44573093783e-06 total time: 120.58638668060303
avg loss validation: 0.000686753128775 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3790.2811528297993, 3783.4395695487578, 3786.3022846608851, 3789.7482948816141, 3789.8286267669951]
mean return 3787.91998574
std of return 2.6552521892
DAGGER NUMBER: 8 of: 200
learning rate: 0.0114707866935
pre_adv_loss 0.000871027037453 post_adv_loss 0.00119263970092 diff 0.000321612663462
Epoch:  75  avg train loss: 0.00119209623776  Reg Loss: 4.55854738016e-06 total time: 8.949405431747437
avg loss validation: 0.000881209913385 failed count: 0
learning rate: 0.011396057646
pre_adv_loss 0.000855648112256 post_adv_loss 0.00117051862908 diff 0.000314870516825
Epoch:  76  avg train loss: 0.00117091074305  Reg Loss: 4.50101233315e-06 total time: 21.299894094467163
avg loss validation: 0.000824955218817 failed count: 0
learning rate: 0.0113227703414
pre_adv_loss 0.000864216280861 post_adv_loss 0.00117907426191 diff 0.000314857981053
Epoch:  77  avg train loss: 0.00117874791613  Reg Loss: 4.44917475514e-06 total time: 33.69206357002258
avg loss validation: 0.000884989895749 failed count: 1
learning rate: 0.0112508790093
pre_adv_loss 0.000840685197326 post_adv_loss 0.00115831984798 diff 0.000317634650654
Epoch:  78  avg train loss: 0.00115792633647  Reg Loss: 4.39790571491e-06 total time: 42.62841582298279
avg loss validation: 0.000571369030315 failed count: 0
learning rate: 0.0111803398875
pre_adv_loss 0.000829050968483 post_adv_loss 0.00113597310206 diff 0.000306922133578
Epoch:  79  avg train loss: 0.00113556245793  Reg Loss: 4.34580322953e-06 total time: 54.74299359321594
avg loss validation: 0.000701451535973 failed count: 1
learning rate: 0.0111111111111
pre_adv_loss 0.000815801913894 post_adv_loss 0.00113208990528 diff 0.000316287991391
Epoch:  80  avg train loss: 0.00113193250249  Reg Loss: 4.29558102657e-06 total time: 63.6470091342926
avg loss validation: 0.00061740872823 failed count: 2
learning rate: 0.0110431526075
pre_adv_loss 0.000773588014161 post_adv_loss 0.00107528311161 diff 0.000301695097453
Epoch:  81  avg train loss: 0.00107449508949  Reg Loss: 4.24362119623e-06 total time: 72.53137516975403
avg loss validation: 0.000417228862558 failed count: 0
learning rate: 0.010976425999
pre_adv_loss 0.00077843575143 post_adv_loss 0.00108418281039 diff 0.00030574705896
Epoch:  82  avg train loss: 0.00108325143346  Reg Loss: 4.19646099815e-06 total time: 84.70495343208313
avg loss validation: 0.000402581534501 failed count: 0
learning rate: 0.0109108945118
pre_adv_loss 0.000762105161458 post_adv_loss 0.00106550290796 diff 0.000303397746504
Epoch:  83  avg train loss: 0.00106567392875  Reg Loss: 4.15351669594e-06 total time: 96.97148370742798
avg loss validation: 0.000963537457547 failed count: 1
learning rate: 0.0108465228909
pre_adv_loss 0.000759455548102 post_adv_loss 0.00106120010353 diff 0.000301744555423
Epoch:  84  avg train loss: 0.00106128346962  Reg Loss: 4.10580505036e-06 total time: 105.8702073097229
avg loss validation: 0.000827656992652 failed count: 2
learning rate: 0.0107832773203
pre_adv_loss 0.000778860427693 post_adv_loss 0.001087203212 diff 0.000308342784307
Epoch:  85  avg train loss: 0.0010868956798  Reg Loss: 4.06566971039e-06 total time: 114.8443124294281
avg loss validation: 0.000867032918116 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
