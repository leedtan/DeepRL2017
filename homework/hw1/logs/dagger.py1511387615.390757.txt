3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) 
[GCC 7.2.0]
using old dagger data
using pre trained model
learning rate: 0.04472135955
pre_adv_loss 0.0617490261295 post_adv_loss 0.0631579960894 diff 0.00140896995991
Epoch:  4  avg train loss: 0.0630337264578  Reg Loss: 4.72127813973e-05 total time: 7.041403293609619
avg loss validation: 0.0129549960955 failed count: 0
learning rate: 0.0408248290464
pre_adv_loss 0.0107005146693 post_adv_loss 0.0116448027265 diff 0.000944288057205
Epoch:  5  avg train loss: 0.0116360896683  Reg Loss: 2.43691673649e-05 total time: 16.812492609024048
avg loss validation: 0.00967398096928 failed count: 0
learning rate: 0.0377964473009
pre_adv_loss 0.00730399573706 post_adv_loss 0.00808147505071 diff 0.000777479313648
Epoch:  6  avg train loss: 0.00807593442569  Reg Loss: 1.92924370055e-05 total time: 26.08657193183899
avg loss validation: 0.00785367178221 failed count: 0
learning rate: 0.0353553390593
pre_adv_loss 0.00588147904094 post_adv_loss 0.00661245880331 diff 0.000730979762368
Epoch:  7  avg train loss: 0.00661183275264  Reg Loss: 1.68284103922e-05 total time: 35.64576458930969
avg loss validation: 0.00519075449217 failed count: 0
learning rate: 0.0333333333333
pre_adv_loss 0.00477961470666 post_adv_loss 0.00545344637125 diff 0.000673831664591
Epoch:  8  avg train loss: 0.00545582142021  Reg Loss: 1.51685714976e-05 total time: 44.90343713760376
avg loss validation: 0.00386410214399 failed count: 0
learning rate: 0.0316227766017
pre_adv_loss 0.00419445211375 post_adv_loss 0.00482993922398 diff 0.000635487110236
Epoch:  9  avg train loss: 0.00483211094847  Reg Loss: 1.39235682454e-05 total time: 54.25012135505676
avg loss validation: 0.00410275394295 failed count: 1
learning rate: 0.0301511344578
pre_adv_loss 0.00392195256158 post_adv_loss 0.00453886964138 diff 0.000616917079799
Epoch:  10  avg train loss: 0.00453650262545  Reg Loss: 1.29473940344e-05 total time: 60.286935329437256
avg loss validation: 0.00280012771207 failed count: 0
learning rate: 0.0288675134595
pre_adv_loss 0.00323914478007 post_adv_loss 0.00380217293654 diff 0.000563028156475
Epoch:  11  avg train loss: 0.00379915762817  Reg Loss: 1.20900144215e-05 total time: 69.9642379283905
avg loss validation: 0.00259389833011 failed count: 0
learning rate: 0.0277350098113
pre_adv_loss 0.0030852746497 post_adv_loss 0.00364697314099 diff 0.000561698491287
Epoch:  12  avg train loss: 0.0036475617964  Reg Loss: 1.13650540466e-05 total time: 79.82831382751465
avg loss validation: 0.00331649853514 failed count: 1
learning rate: 0.0267261241912
pre_adv_loss 0.00300200325047 post_adv_loss 0.0035380319917 diff 0.000536028741229
Epoch:  13  avg train loss: 0.0035393047218  Reg Loss: 1.07333454715e-05 total time: 86.48100876808167
avg loss validation: 0.00226068013329 failed count: 0
learning rate: 0.0258198889747
pre_adv_loss 0.00290684789053 post_adv_loss 0.00344703893071 diff 0.000540191040178
Epoch:  14  avg train loss: 0.00344546872577  Reg Loss: 1.01980958239e-05 total time: 96.17173171043396
avg loss validation: 0.00321372323129 failed count: 1
learning rate: 0.025
pre_adv_loss 0.00250469955429 post_adv_loss 0.00301939653498 diff 0.000514696980692
Epoch:  15  avg train loss: 0.00301623161299  Reg Loss: 9.70569288583e-06 total time: 102.3393805027008
avg loss validation: 0.00338090647326 failed count: 2
learning rate: 0.0242535625036
pre_adv_loss 0.00258334920906 post_adv_loss 0.00309352007004 diff 0.000510170860983
Epoch:  16  avg train loss: 0.00309363088419  Reg Loss: 9.28121009701e-06 total time: 108.38269853591919
avg loss validation: 0.00163515015928 failed count: 0
learning rate: 0.0235702260396
pre_adv_loss 0.00231135965458 post_adv_loss 0.00280076496147 diff 0.000489405306888
Epoch:  17  avg train loss: 0.00279991986829  Reg Loss: 8.88491571956e-06 total time: 117.70991253852844
avg loss validation: 0.00235107505668 failed count: 1
learning rate: 0.0229415733871
pre_adv_loss 0.00237275843644 post_adv_loss 0.00287484906269 diff 0.000502090626248
Epoch:  18  avg train loss: 0.00287350898357  Reg Loss: 8.52778961165e-06 total time: 123.786780834198
avg loss validation: 0.00271750899912 failed count: 2
learning rate: 0.022360679775
pre_adv_loss 0.00206340317624 post_adv_loss 0.00252600578252 diff 0.000462602606283
Epoch:  19  avg train loss: 0.00252459892731  Reg Loss: 8.2103849942e-06 total time: 129.83364963531494
avg loss validation: 0.00115705140194 failed count: 0
learning rate: 0.0218217890236
pre_adv_loss 0.00230169385797 post_adv_loss 0.00279267417634 diff 0.000490980318376
Epoch:  20  avg train loss: 0.00279115778809  Reg Loss: 7.92143645958e-06 total time: 139.73835015296936
avg loss validation: 0.00191707543412 failed count: 1
learning rate: 0.0213200716356
pre_adv_loss 0.0020291856056 post_adv_loss 0.00249091148911 diff 0.000461725883503
Epoch:  21  avg train loss: 0.00248993218289  Reg Loss: 7.66991988436e-06 total time: 145.78608059883118
avg loss validation: 0.00138181067621 failed count: 2
learning rate: 0.0208514414057
pre_adv_loss 0.00211389800783 post_adv_loss 0.00259059282643 diff 0.000476694818602
Epoch:  22  avg train loss: 0.00259078986636  Reg Loss: 7.44951623445e-06 total time: 151.84846115112305
avg loss validation: 0.00193129579759 failed count: 3
loading and building expert policy
obs (1, 11) (1, 11)
loaded and built
threshold: 0
iter 0
100/1000
200/1000
300/1000
400/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
returns [1753.557256945905, 3785.8676942823754, 3793.1129318682842, 1976.0441358361127, 2287.3614785103819]
mean return 2719.18869949
std of return 890.202225218
DAGGER NUMBER: 0 of: 200
learning rate: 0.0204124145232
pre_adv_loss 0.00208021947183 post_adv_loss 0.00256212417702 diff 0.000481904705197
Epoch:  23  avg train loss: 0.00256071498237  Reg Loss: 7.91646176275e-06 total time: 6.890548229217529
avg loss validation: 0.00133511385137 failed count: 0
learning rate: 0.02
pre_adv_loss 0.00216125292904 post_adv_loss 0.002647762614 diff 0.000486509684965
Epoch:  24  avg train loss: 0.00264666838025  Reg Loss: 7.6648481481e-06 total time: 16.822483777999878
avg loss validation: 0.00195393377476 failed count: 1
learning rate: 0.0196116135138
pre_adv_loss 0.00181179516868 post_adv_loss 0.00225734641135 diff 0.000445551242673
Epoch:  25  avg train loss: 0.00225909530528  Reg Loss: 7.43221852954e-06 total time: 23.227030038833618
avg loss validation: 0.00195749723595 failed count: 2
learning rate: 0.019245008973
pre_adv_loss 0.00176043765128 post_adv_loss 0.00220615090156 diff 0.000445713250277
Epoch:  26  avg train loss: 0.00220740235191  Reg Loss: 7.20168415612e-06 total time: 29.73290705680847
avg loss validation: 0.00148835281642 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
iter 3
100/1000
200/1000
300/1000
400/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [1765.9549512615395, 3792.45276888131, 1917.4822211268461, 1769.6466165081665, 3794.1214020380025]
mean return 2607.93159196
std of return 969.381594765
DAGGER NUMBER: 1 of: 200
learning rate: 0.0188982236505
pre_adv_loss 0.00190184832417 post_adv_loss 0.0023667100088 diff 0.000464861684623
Epoch:  27  avg train loss: 0.00236670229904  Reg Loss: 7.66420813441e-06 total time: 7.425880670547485
avg loss validation: 0.0011641226612 failed count: 0
learning rate: 0.0185695338177
pre_adv_loss 0.00185061663104 post_adv_loss 0.00230505758552 diff 0.000454440954475
Epoch:  28  avg train loss: 0.00230476699321  Reg Loss: 7.41708300965e-06 total time: 17.7245135307312
avg loss validation: 0.00121471599912 failed count: 1
learning rate: 0.0182574185835
pre_adv_loss 0.00188751953992 post_adv_loss 0.00233423488182 diff 0.000446715341902
Epoch:  29  avg train loss: 0.00233403378889  Reg Loss: 7.21170276294e-06 total time: 24.304256916046143
avg loss validation: 0.00156399814211 failed count: 2
learning rate: 0.0179605302027
pre_adv_loss 0.00166532314192 post_adv_loss 0.0020972218912 diff 0.00043189874928
Epoch:  30  avg train loss: 0.00209718869974  Reg Loss: 7.00539556791e-06 total time: 30.845505475997925
avg loss validation: 0.000967590346641 failed count: 0
learning rate: 0.0176776695297
pre_adv_loss 0.00172834714951 post_adv_loss 0.00215933464043 diff 0.000430987490918
Epoch:  31  avg train loss: 0.0021590262604  Reg Loss: 6.81724715402e-06 total time: 40.67918014526367
avg loss validation: 0.000951806021193 failed count: 0
learning rate: 0.0174077655956
pre_adv_loss 0.00166149943616 post_adv_loss 0.00209099409715 diff 0.000429494660983
Epoch:  32  avg train loss: 0.00209105130381  Reg Loss: 6.65233680527e-06 total time: 50.510398864746094
avg loss validation: 0.00218818780612 failed count: 1
learning rate: 0.0171498585143
pre_adv_loss 0.00168490574249 post_adv_loss 0.00211905583367 diff 0.000434150091182
Epoch:  33  avg train loss: 0.00211876171592  Reg Loss: 6.52153166703e-06 total time: 57.232760429382324
avg loss validation: 0.000840571136938 failed count: 0
learning rate: 0.0169030850946
pre_adv_loss 0.00150448931095 post_adv_loss 0.00190762444748 diff 0.000403135136532
Epoch:  34  avg train loss: 0.00190770989801  Reg Loss: 6.37826149887e-06 total time: 67.82110834121704
avg loss validation: 0.00131229652832 failed count: 1
learning rate: 0.0166666666667
pre_adv_loss 0.00150826807634 post_adv_loss 0.00192376894983 diff 0.000415500873491
Epoch:  35  avg train loss: 0.00192387006502  Reg Loss: 6.24121927999e-06 total time: 75.68492841720581
avg loss validation: 0.00141686143976 failed count: 2
learning rate: 0.0164398987305
pre_adv_loss 0.00147453091343 post_adv_loss 0.00189017141558 diff 0.000415640502152
Epoch:  36  avg train loss: 0.00189043126048  Reg Loss: 6.13293885176e-06 total time: 82.30341386795044
avg loss validation: 0.0012011159026 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3784.311239796732, 3792.5587205110687, 3792.5130121268517, 3788.4585641706085, 3797.731936766213]
mean return 3791.11469467
std of return 4.49823126889
DAGGER NUMBER: 2 of: 200
learning rate: 0.0162221421131
pre_adv_loss 0.0014814935462 post_adv_loss 0.00188748584794 diff 0.000405992301739
Epoch:  37  avg train loss: 0.00188685860049  Reg Loss: 6.37118832015e-06 total time: 7.137088298797607
avg loss validation: 0.000966453908757 failed count: 0
learning rate: 0.0160128153805
pre_adv_loss 0.00146665900937 post_adv_loss 0.00188647014114 diff 0.000419811131764
Epoch:  38  avg train loss: 0.00188654009672  Reg Loss: 6.23283320518e-06 total time: 17.689738273620605
avg loss validation: 0.00127092126253 failed count: 1
learning rate: 0.0158113883008
pre_adv_loss 0.00141898057697 post_adv_loss 0.00182216703677 diff 0.0004031864598
Epoch:  39  avg train loss: 0.00182236073491  Reg Loss: 6.11271292844e-06 total time: 24.84306025505066
avg loss validation: 0.00137535253443 failed count: 2
learning rate: 0.0156173761889
pre_adv_loss 0.00136617309225 post_adv_loss 0.00176523457374 diff 0.000399061481489
Epoch:  40  avg train loss: 0.00176430750704  Reg Loss: 5.98711879478e-06 total time: 31.919658184051514
avg loss validation: 0.000957571313483 failed count: 0
learning rate: 0.0154303349962
pre_adv_loss 0.00123135868606 post_adv_loss 0.00160480428869 diff 0.000373445602629
Epoch:  41  avg train loss: 0.0016047979859  Reg Loss: 5.87456137315e-06 total time: 42.53073692321777
avg loss validation: 0.00156109249561 failed count: 1
learning rate: 0.0152498570333
pre_adv_loss 0.00135404466617 post_adv_loss 0.00174986938211 diff 0.000395824715943
Epoch:  42  avg train loss: 0.00174986746317  Reg Loss: 5.76985218382e-06 total time: 49.51490807533264
avg loss validation: 0.00192405529313 failed count: 2
learning rate: 0.0150755672289
pre_adv_loss 0.00119908052481 post_adv_loss 0.0015712578653 diff 0.000372177340492
Epoch:  43  avg train loss: 0.00157157622071  Reg Loss: 5.67145139023e-06 total time: 56.58400630950928
avg loss validation: 0.00112520027833 failed count: 3
threshold: 0.0
iter 0
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 1
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
iter 2
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 3
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
iter 4
100/1000
200/1000
300/1000
400/1000
500/1000
600/1000
700/1000
800/1000
900/1000
1000/1000
returns [3795.4558103860163, 2253.7806679221217, 3801.1588685631932, 3795.3327234372196, 3790.7494649053551]
mean return 3487.29550704
std of return 616.766255857
DAGGER NUMBER: 3 of: 200
learning rate: 0.01490711985
pre_adv_loss 0.00127208060664 post_adv_loss 0.00165805925441 diff 0.000385978647764
Epoch:  44  avg train loss: 0.00165786260025  Reg Loss: 5.88373389579e-06 total time: 8.213726282119751
avg loss validation: 0.00109172990713 failed count: 0
learning rate: 0.0147441956155
pre_adv_loss 0.00113422083164 post_adv_loss 0.00149644567312 diff 0.000362224841475
Epoch:  45  avg train loss: 0.00149652294649  Reg Loss: 5.76304082863e-06 total time: 19.571973085403442
avg loss validation: 0.0010462302583 failed count: 0
learning rate: 0.0145864991498
pre_adv_loss 0.00128336726626 post_adv_loss 0.00166838984739 diff 0.000385022581127
Epoch:  46  avg train loss: 0.001668040707  Reg Loss: 5.66038652163e-06 total time: 30.492733001708984
avg loss validation: 0.00100010571726 failed count: 0
learning rate: 0.0144337567297
pre_adv_loss 0.00125678347488 post_adv_loss 0.00163039612474 diff 0.000373612649862
Epoch:  47  avg train loss: 0.00163068031881  Reg Loss: 5.57153071031e-06 total time: 42.863959074020386
avg loss validation: 0.000793997935594 failed count: 0
learning rate: 0.0142857142857
